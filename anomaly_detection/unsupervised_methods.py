#!/usr/bin/env python3
"""
æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•
åŒ…å«è‡ªç¼–ç å™¨ã€å­¤ç«‹æ£®æ—ã€One-Class SVMç­‰æ–¹æ³•
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from pathlib import Path
import yaml
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class AnomalyDataset(Dataset):
    """å¼‚å¸¸æ£€æµ‹æ•°æ®é›†"""
    
    def __init__(self, signals: List[np.ndarray], max_length: int = 2048):
        self.max_length = max_length
        
        # é¢„å¤„ç†ä¿¡å·
        self.processed_signals = []
        for signal in signals:
            # é‡é‡‡æ ·åˆ°å›ºå®šé•¿åº¦
            if len(signal) > max_length:
                indices = np.linspace(0, len(signal)-1, max_length, dtype=int)
                signal = signal[indices]
            else:
                signal = np.pad(signal, (0, max_length - len(signal)), 'constant')
            
            # æ ‡å‡†åŒ–
            signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)
            
            self.processed_signals.append(signal)
        
        print(f"ğŸ“Š å¼‚å¸¸æ£€æµ‹æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(self.processed_signals)} ä¸ªä¿¡å·")
    
    def __len__(self):
        return len(self.processed_signals)
    
    def __getitem__(self, idx):
        signal = self.processed_signals[idx]
        return torch.FloatTensor(signal).unsqueeze(0)

class DeepAutoEncoder(nn.Module):
    """æ·±å±‚è‡ªç¼–ç å™¨"""
    
    def __init__(self, input_length: int = 2048, encoding_dim: int = 64):
        super(DeepAutoEncoder, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            # ç¬¬ä¸€å±‚
            nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            # ç¬¬äºŒå±‚
            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            # ç¬¬ä¸‰å±‚
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            
            # ç¬¬å››å±‚
            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(encoding_dim)
        )
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            # ä¸Šé‡‡æ ·å¼€å§‹
            nn.ConvTranspose1d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            
            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            
            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            
            nn.ConvTranspose1d(32, 1, kernel_size=7, stride=2, padding=3, output_padding=1),
            nn.Tanh()
        )
    
    def forward(self, x):
        # ç¼–ç 
        encoded = self.encoder(x)
        
        # è§£ç 
        decoded = self.decoder(encoded)
        
        # è°ƒæ•´è¾“å‡ºé•¿åº¦
        if decoded.size(-1) != x.size(-1):
            decoded = nn.functional.interpolate(decoded, size=x.size(-1), 
                                              mode='linear', align_corners=False)
        
        return decoded, encoded

class VariationalAutoEncoder(nn.Module):
    """å˜åˆ†è‡ªç¼–ç å™¨"""
    
    def __init__(self, input_length: int = 2048, latent_dim: int = 32):
        super(VariationalAutoEncoder, self).__init__()
        
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨
        self.encoder_conv = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(64)
        )
        
        # å‡å€¼å’Œæ–¹å·®
        self.fc_mu = nn.Linear(128 * 64, latent_dim)
        self.fc_logvar = nn.Linear(128 * 64, latent_dim)
        
        # è§£ç å™¨
        self.decoder_fc = nn.Linear(latent_dim, 128 * 64)
        self.decoder_conv = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose1d(32, 1, kernel_size=7, stride=2, padding=3, output_padding=1),
            nn.Tanh()
        )
    
    def encode(self, x):
        h = self.encoder_conv(x)
        h = h.view(h.size(0), -1)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = self.decoder_fc(z)
        h = h.view(h.size(0), 128, 64)
        return self.decoder_conv(h)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        
        # è°ƒæ•´è¾“å‡ºé•¿åº¦
        if recon.size(-1) != x.size(-1):
            recon = nn.functional.interpolate(recon, size=x.size(-1), 
                                            mode='linear', align_corners=False)
        
        return recon, mu, logvar

class UnsupervisedAnomalyDetector:
    """æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.models_path = self.output_path.parent / 'models'
        self.models_path.mkdir(exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.results = {}
    
    def load_normal_data(self) -> List[np.ndarray]:
        """åŠ è½½æ­£å¸¸æ•°æ®ç”¨äºè®­ç»ƒ"""
        print("ğŸ“‚ åŠ è½½æ­£å¸¸æ•°æ®...")
        
        # ä»å¢å¼ºæ•°æ®åŠ è½½å™¨åŠ è½½æ•°æ®
        import sys
        sys.path.append(str(Path(__file__).parent.parent / 'data_processing'))
        from enhanced_data_loader import EnhancedMotorDataLoader
        
        config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
        loader = EnhancedMotorDataLoader(str(config_path))
        dataset, _ = loader.load_comprehensive_dataset(enable_augmentation=False)
        
        # åªä½¿ç”¨æ­£å¸¸æ•°æ®è®­ç»ƒ
        normal_signals = dataset['single_sensor']['normal']
        
        print(f"âœ… åŠ è½½æ­£å¸¸æ•°æ®: {len(normal_signals)} ä¸ªæ ·æœ¬")
        return normal_signals
    
    def load_test_data(self) -> Tuple[List[np.ndarray], List[str]]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        
        import sys
        sys.path.append(str(Path(__file__).parent.parent / 'data_processing'))
        from enhanced_data_loader import EnhancedMotorDataLoader
        
        config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
        loader = EnhancedMotorDataLoader(str(config_path))
        dataset, _ = loader.load_comprehensive_dataset(enable_augmentation=False)
        
        # æ”¶é›†æ‰€æœ‰æµ‹è¯•æ•°æ®
        all_signals = []
        all_labels = []
        
        for state in ['normal', 'spark', 'vibrate']:
            signals = dataset['single_sensor'][state]
            all_signals.extend(signals)
            all_labels.extend([state] * len(signals))
        
        print(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {len(all_signals)} ä¸ªæ ·æœ¬")
        return all_signals, all_labels
    
    def train_autoencoder(self, normal_signals: List[np.ndarray], 
                         model_type: str = "deep") -> nn.Module:
        """è®­ç»ƒè‡ªç¼–ç å™¨"""
        print(f"ğŸ¯ è®­ç»ƒ{model_type}è‡ªç¼–ç å™¨...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = AnomalyDataset(normal_signals)
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
        
        # åˆ›å»ºæ¨¡å‹
        if model_type == "deep":
            model = DeepAutoEncoder().to(self.device)
        else:
            model = VariationalAutoEncoder().to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # è®­ç»ƒ
        model.train()
        for epoch in range(50):
            total_loss = 0
            
            for batch_signals in dataloader:
                batch_signals = batch_signals.to(self.device)
                
                optimizer.zero_grad()
                
                if model_type == "deep":
                    reconstructed, _ = model(batch_signals)
                    loss = nn.MSELoss()(reconstructed, batch_signals)
                else:
                    reconstructed, mu, logvar = model(batch_signals)
                    recon_loss = nn.MSELoss()(reconstructed, batch_signals)
                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                    loss = recon_loss + 0.001 * kl_loss
                
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch}, Loss: {total_loss/len(dataloader):.4f}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.models_path / f'{model_type}_autoencoder.pth'
        torch.save(model.state_dict(), model_path)
        
        print(f"âœ… {model_type}è‡ªç¼–ç å™¨è®­ç»ƒå®Œæˆ")
        return model
    
    def detect_anomalies_autoencoder(self, model: nn.Module, test_signals: List[np.ndarray], 
                                   model_type: str = "deep") -> np.ndarray:
        """ä½¿ç”¨è‡ªç¼–ç å™¨æ£€æµ‹å¼‚å¸¸"""
        print("ğŸ” ä½¿ç”¨è‡ªç¼–ç å™¨æ£€æµ‹å¼‚å¸¸...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = AnomalyDataset(test_signals)
        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
        
        model.eval()
        reconstruction_errors = []
        
        with torch.no_grad():
            for batch_signals in test_loader:
                batch_signals = batch_signals.to(self.device)
                
                if model_type == "deep":
                    reconstructed, _ = model(batch_signals)
                else:
                    reconstructed, _, _ = model(batch_signals)
                
                # è®¡ç®—é‡å»ºè¯¯å·®
                errors = torch.mean((batch_signals - reconstructed) ** 2, dim=(1, 2))
                reconstruction_errors.extend(errors.cpu().numpy())
        
        reconstruction_errors = np.array(reconstruction_errors)
        
        # ä½¿ç”¨é˜ˆå€¼æ£€æµ‹å¼‚å¸¸ï¼ˆåŸºäºæ­£å¸¸æ•°æ®çš„é‡å»ºè¯¯å·®åˆ†å¸ƒï¼‰
        threshold = np.percentile(reconstruction_errors[:len(test_signals)//3], 95)  # å‡è®¾å‰1/3æ˜¯æ­£å¸¸æ•°æ®
        anomaly_scores = reconstruction_errors > threshold
        
        return anomaly_scores.astype(int)
    
    def detect_anomalies_isolation_forest(self, normal_signals: List[np.ndarray], 
                                        test_signals: List[np.ndarray]) -> np.ndarray:
        """ä½¿ç”¨å­¤ç«‹æ£®æ—æ£€æµ‹å¼‚å¸¸"""
        print("ğŸŒ² ä½¿ç”¨å­¤ç«‹æ£®æ—æ£€æµ‹å¼‚å¸¸...")
        
        # æå–ç»Ÿè®¡ç‰¹å¾
        def extract_features(signals):
            features = []
            for signal in signals:
                feat = [
                    np.mean(signal), np.std(signal), np.var(signal),
                    np.min(signal), np.max(signal), np.median(signal),
                    np.percentile(signal, 25), np.percentile(signal, 75),
                    np.sum(np.abs(np.diff(signal))),  # æ€»å˜åŒ–é‡
                    len(np.where(np.diff(np.sign(signal)))[0]),  # é›¶äº¤å‰æ•°
                ]
                features.append(feat)
            return np.array(features)
        
        # æå–ç‰¹å¾
        normal_features = extract_features(normal_signals)
        test_features = extract_features(test_signals)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        normal_features_scaled = scaler.fit_transform(normal_features)
        test_features_scaled = scaler.transform(test_features)
        
        # è®­ç»ƒå­¤ç«‹æ£®æ—
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        iso_forest.fit(normal_features_scaled)
        
        # é¢„æµ‹å¼‚å¸¸
        predictions = iso_forest.predict(test_features_scaled)
        # è½¬æ¢ä¸º0/1æ ‡ç­¾ï¼ˆ-1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸ï¼‰
        anomaly_scores = (predictions == -1).astype(int)
        
        return anomaly_scores
    
    def detect_anomalies_one_class_svm(self, normal_signals: List[np.ndarray], 
                                     test_signals: List[np.ndarray]) -> np.ndarray:
        """ä½¿ç”¨One-Class SVMæ£€æµ‹å¼‚å¸¸"""
        print("ğŸ¯ ä½¿ç”¨One-Class SVMæ£€æµ‹å¼‚å¸¸...")
        
        # æå–ç‰¹å¾ï¼ˆåŒå­¤ç«‹æ£®æ—ï¼‰
        def extract_features(signals):
            features = []
            for signal in signals:
                feat = [
                    np.mean(signal), np.std(signal), np.var(signal),
                    np.min(signal), np.max(signal), np.median(signal),
                    np.percentile(signal, 25), np.percentile(signal, 75),
                    np.sum(np.abs(np.diff(signal))),
                    len(np.where(np.diff(np.sign(signal)))[0]),
                ]
                features.append(feat)
            return np.array(features)
        
        # æå–ç‰¹å¾
        normal_features = extract_features(normal_signals)
        test_features = extract_features(test_signals)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        normal_features_scaled = scaler.fit_transform(normal_features)
        test_features_scaled = scaler.transform(test_features)
        
        # è®­ç»ƒOne-Class SVM
        oc_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')
        oc_svm.fit(normal_features_scaled)
        
        # é¢„æµ‹å¼‚å¸¸
        predictions = oc_svm.predict(test_features_scaled)
        anomaly_scores = (predictions == -1).astype(int)
        
        return anomaly_scores
