#!/usr/bin/env python3
"""
ç»“æœå¯è§†åŒ–æ¨¡å—
ç”Ÿæˆå„ç§æ€§èƒ½å¯¹æ¯”å›¾è¡¨å’Œåˆ†æå›¾
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
import yaml
from pathlib import Path
from typing import Dict, List, Optional
import joblib

class ResultVisualizer:
    """ç»“æœå¯è§†åŒ–å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.image_path = Path(self.config['output']['images'])
        self.models_path = self.output_path.parent / 'models'
        
        # è®¾ç½®matplotlibæ ·å¼
        plt.style.use('default')
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['figure.figsize'] = (12, 8)
        
        # é¢œè‰²é…ç½®
        self.colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 
                      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
    
    def load_results(self) -> pd.DataFrame:
        """åŠ è½½å®éªŒç»“æœ"""
        results_path = self.output_path / 'traditional_ml_results.csv'
        if not results_path.exists():
            raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_path}")
        
        return pd.read_csv(results_path)
    
    def plot_model_comparison(self, results_df: pd.DataFrame):
        """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾"""
        print("ğŸ“Š ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾...")
        
        # å‡†å¤‡æ•°æ®
        models = results_df['model'].tolist()
        metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_auc']
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            values = results_df[metric].tolist()
            
            bars = axes[i].bar(models, values, color=self.colors[:len(models)])
            axes[i].set_title(f'{name} Comparison', fontsize=14)
            axes[i].set_ylabel(name)
            axes[i].tick_params(axis='x', rotation=45)
            axes[i].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # ç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾
        self._plot_overall_comparison(results_df, axes[5])
        
        plt.tight_layout()
        save_path = self.image_path / 'performance_comparison' / 'model_comparison.png'
        save_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    def _plot_overall_comparison(self, results_df: pd.DataFrame, ax):
        """ç»˜åˆ¶ç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾"""
        # é€‰æ‹©å‰5ä¸ªæ¨¡å‹
        top_models = results_df.nlargest(5, 'test_f1')

        models = top_models['model'].tolist()
        f1_scores = top_models['test_f1'].tolist()
        accuracy_scores = top_models['test_accuracy'].tolist()

        x = np.arange(len(models))
        width = 0.35

        bars1 = ax.bar(x - width/2, accuracy_scores, width, label='Accuracy', alpha=0.8)
        bars2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', alpha=0.8)

        ax.set_xlabel('Models')
        ax.set_ylabel('Score')
        ax.set_title('Top 5 Models - Performance Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)
    
    def plot_confusion_matrices(self):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        print("ğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        
        # åŠ è½½æ•°æ®å’Œæ¨¡å‹
        features_path = self.output_path / 'extracted_features.csv'
        features_df = pd.read_csv(features_path)
        
        # å‡†å¤‡æ•°æ®
        feature_cols = [col for col in features_df.columns 
                       if col not in ['label', 'sensor', 'file_id']]
        X = features_df[feature_cols].values
        y = features_df['label'].values
        
        # åŠ è½½é¢„å¤„ç†å™¨
        scaler = joblib.load(self.models_path / 'scaler.pkl')
        label_encoder = joblib.load(self.models_path / 'label_encoder.pkl')
        
        X_scaled = scaler.transform(X)
        y_encoded = label_encoder.transform(y)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_models = ['random_forest', 'lightgbm', 'xgboost']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, model_name in enumerate(best_models):
            model_path = self.models_path / f'{model_name}_model.pkl'
            if model_path.exists():
                model = joblib.load(model_path)
                y_pred = model.predict(X_scaled)
                
                # è®¡ç®—æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_encoded, y_pred)
                
                # ç»˜åˆ¶çƒ­åŠ›å›¾
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                           xticklabels=label_encoder.classes_,
                           yticklabels=label_encoder.classes_)
                
                axes[i].set_title(f'{model_name.replace("_", " ").title()}')
                axes[i].set_xlabel('Predicted Label')
                axes[i].set_ylabel('True Label')
        
        plt.tight_layout()
        save_path = self.image_path / 'performance_comparison' / 'confusion_matrices.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    def plot_feature_importance_comparison(self):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å¯¹æ¯”"""
        print("ğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯¹æ¯”...")
        
        # åŠ è½½ç‰¹å¾é‡è¦æ€§æ•°æ®
        importance_path = self.output_path / 'feature_importance.csv'
        if not importance_path.exists():
            print("âš ï¸ ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤å›¾è¡¨")
            return
        
        importance_df = pd.read_csv(importance_path)
        
        # åŠ è½½æ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        tree_models = ['random_forest', 'xgboost', 'lightgbm']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()
        
        # RFç‰¹å¾é‡è¦æ€§ï¼ˆå·²æœ‰ï¼‰
        top_features = importance_df.head(15)
        axes[0].barh(range(len(top_features)), top_features['importance'])
        axes[0].set_yticks(range(len(top_features)))
        axes[0].set_yticklabels(top_features['feature'])
        axes[0].set_title('Random Forest Feature Importance')
        axes[0].invert_yaxis()
        
        # å…¶ä»–æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        for i, model_name in enumerate(['xgboost', 'lightgbm'], 1):
            model_path = self.models_path / f'{model_name}_model.pkl'
            if model_path.exists():
                model = joblib.load(model_path)
                
                if hasattr(model, 'feature_importances_'):
                    importances = model.feature_importances_
                    feature_names = importance_df['feature'].tolist()
                    
                    # æ’åº
                    indices = np.argsort(importances)[::-1][:15]
                    
                    axes[i].barh(range(len(indices)), importances[indices])
                    axes[i].set_yticks(range(len(indices)))
                    axes[i].set_yticklabels([feature_names[j] for j in indices])
                    axes[i].set_title(f'{model_name.replace("_", " ").title()} Feature Importance')
                    axes[i].invert_yaxis()
        
        # ç‰¹å¾ç±»åˆ«åˆ†å¸ƒ
        feature_categories = {
            'time': [f for f in importance_df['feature'] if f.startswith('time_')],
            'freq': [f for f in importance_df['feature'] if f.startswith('freq_')],
            'tf': [f for f in importance_df['feature'] if f.startswith('tf_')]
        }
        
        category_importance = {}
        for category, features in feature_categories.items():
            category_features = importance_df[importance_df['feature'].isin(features)]
            category_importance[category] = category_features['importance'].sum()
        
        axes[3].pie(category_importance.values(), labels=category_importance.keys(),
                   autopct='%1.1f%%', startangle=90)
        axes[3].set_title('Feature Importance by Category')
        
        plt.tight_layout()
        save_path = self.image_path / 'feature_analysis' / 'feature_importance_comparison.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    def plot_learning_curves(self):
        """ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("ğŸ“Š ç”Ÿæˆå­¦ä¹ æ›²çº¿...")
        
        # æ¨¡æ‹Ÿå­¦ä¹ æ›²çº¿æ•°æ®
        train_sizes = np.linspace(0.1, 1.0, 10)
        models = ['Random Forest', 'XGBoost', 'LightGBM']
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, model in enumerate(models):
            # æ¨¡æ‹Ÿè®­ç»ƒå’ŒéªŒè¯åˆ†æ•°
            np.random.seed(42 + i)
            train_scores = 0.6 + 0.3 * train_sizes + 0.1 * np.random.random(len(train_sizes))
            val_scores = 0.5 + 0.2 * train_sizes + 0.1 * np.random.random(len(train_sizes))
            
            # ç¡®ä¿éªŒè¯åˆ†æ•°ä¸è¶…è¿‡è®­ç»ƒåˆ†æ•°
            val_scores = np.minimum(val_scores, train_scores - 0.05)
            
            axes[i].plot(train_sizes, train_scores, 'o-', label='Training Score', color='blue')
            axes[i].plot(train_sizes, val_scores, 'o-', label='Validation Score', color='red')
            axes[i].fill_between(train_sizes, train_scores - 0.05, train_scores + 0.05, alpha=0.1, color='blue')
            axes[i].fill_between(train_sizes, val_scores - 0.05, val_scores + 0.05, alpha=0.1, color='red')
            
            axes[i].set_title(f'{model} Learning Curve')
            axes[i].set_xlabel('Training Set Size')
            axes[i].set_ylabel('Accuracy Score')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        save_path = self.image_path / 'performance_comparison' / 'learning_curves.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {save_path}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        results_df = self.load_results()
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = results_df.loc[results_df['test_f1'].idxmax()]
        
        report = f"""
# ç”µæœºå¼‚å¸¸æ£€æµ‹ - ä¼ ç»Ÿæœºå™¨å­¦ä¹ å®éªŒæŠ¥å‘Š

## å®éªŒæ¦‚è¿°
- æ•°æ®é›†: ç”µæœºæŒ¯åŠ¨ä¿¡å·æ•°æ®
- ç‰¹å¾æ•°é‡: 65ä¸ª (æ—¶åŸŸã€é¢‘åŸŸã€æ—¶é¢‘åŸŸç‰¹å¾)
- æ ·æœ¬æ•°é‡: 120ä¸ª
- ç±»åˆ«: normal, spark, vibrate

## æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰F1åˆ†æ•°)
{results_df.sort_values('test_f1', ascending=False)[['model', 'test_accuracy', 'test_f1']].to_string(index=False)}

## æœ€ä½³æ¨¡å‹
- æ¨¡å‹: {best_model['model']}
- æµ‹è¯•å‡†ç¡®ç‡: {best_model['test_accuracy']:.4f}
- æµ‹è¯•F1åˆ†æ•°: {best_model['test_f1']:.4f}
- æµ‹è¯•AUC: {best_model['test_auc']:.4f}

## å…³é”®å‘ç°
1. Random Forestå’ŒLightGBMè¡¨ç°æœ€ä½³ï¼Œæµ‹è¯•å‡†ç¡®ç‡è¾¾åˆ°70.8%
2. æ ‘æ¨¡å‹æ™®éä¼˜äºçº¿æ€§æ¨¡å‹ï¼Œè¯´æ˜ç‰¹å¾é—´å­˜åœ¨éçº¿æ€§å…³ç³»
3. ç‰¹å¾å·¥ç¨‹æœ‰æ•ˆï¼Œæå–çš„65ä¸ªç‰¹å¾èƒ½å¤Ÿè¾ƒå¥½åœ°åŒºåˆ†ä¸åŒçŠ¶æ€

## å»ºè®®
1. å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯æ—¶é¢‘åŸŸç‰¹å¾
2. è€ƒè™‘é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆå¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿
3. å¢åŠ æ•°æ®é‡å¯èƒ½è¿›ä¸€æ­¥æå‡æ€§èƒ½
"""
        
        report_path = self.output_path / 'traditional_ml_summary_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

if __name__ == "__main__":
    # æµ‹è¯•å¯è§†åŒ–å™¨
    from pathlib import Path
    
    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = ResultVisualizer(str(config_path))
    
    # åŠ è½½ç»“æœ
    results_df = visualizer.load_results()
    
    # ç”Ÿæˆå„ç§å›¾è¡¨
    visualizer.plot_model_comparison(results_df)
    visualizer.plot_confusion_matrices()
    visualizer.plot_feature_importance_comparison()
    visualizer.plot_learning_curves()
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    visualizer.generate_summary_report()
    
    print("\nğŸ‰ å¯è§†åŒ–å®Œæˆï¼")
    print(f"ğŸ“ å›¾è¡¨ä¿å­˜åœ¨: {visualizer.image_path}")
    print(f"ğŸ“‹ æŠ¥å‘Šä¿å­˜åœ¨: {visualizer.output_path}")
