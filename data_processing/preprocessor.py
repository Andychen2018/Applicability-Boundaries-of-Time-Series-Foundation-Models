#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
åŒ…å«ä¿¡å·å»å™ªã€æ»¤æ³¢ã€æ ‡å‡†åŒ–ç­‰é¢„å¤„ç†åŠŸèƒ½
"""

import numpy as np
import pandas as pd
from scipy import signal
from scipy.signal import butter, filtfilt, savgol_filter
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from typing import Dict, List, Tuple, Optional
import yaml
from pathlib import Path

class SignalPreprocessor:
    """ä¿¡å·é¢„å¤„ç†å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.sampling_rate = self.config['data']['sampling_rate']
        self.output_path = Path(self.config['output']['tables'])
    
    def remove_outliers(self, signal: np.ndarray, method: str = 'iqr', 
                       threshold: float = 3.0) -> np.ndarray:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        if method == 'iqr':
            Q1 = np.percentile(signal, 25)
            Q3 = np.percentile(signal, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºè¾¹ç•Œå€¼
            signal_clean = np.clip(signal, lower_bound, upper_bound)
            
        elif method == 'zscore':
            mean_val = np.mean(signal)
            std_val = np.std(signal)
            z_scores = np.abs((signal - mean_val) / std_val)
            
            # å°†è¶…è¿‡é˜ˆå€¼çš„ç‚¹æ›¿æ¢ä¸ºå‡å€¼
            signal_clean = signal.copy()
            outlier_mask = z_scores > threshold
            signal_clean[outlier_mask] = mean_val
            
        else:
            raise ValueError(f"Unknown outlier removal method: {method}")
        
        return signal_clean
    
    def apply_filter(self, signal: np.ndarray, filter_type: str = 'lowpass',
                    cutoff: float = 1000, order: int = 4) -> np.ndarray:
        """åº”ç”¨æ•°å­—æ»¤æ³¢å™¨"""
        nyquist = self.sampling_rate / 2
        
        if filter_type == 'lowpass':
            b, a = butter(order, cutoff / nyquist, btype='low')
        elif filter_type == 'highpass':
            b, a = butter(order, cutoff / nyquist, btype='high')
        elif filter_type == 'bandpass':
            if isinstance(cutoff, (list, tuple)) and len(cutoff) == 2:
                low, high = cutoff
                b, a = butter(order, [low / nyquist, high / nyquist], btype='band')
            else:
                raise ValueError("Bandpass filter requires two cutoff frequencies")
        else:
            raise ValueError(f"Unknown filter type: {filter_type}")
        
        # ä½¿ç”¨é›¶ç›¸ä½æ»¤æ³¢
        filtered_signal = filtfilt(b, a, signal)
        return filtered_signal
    
    def smooth_signal(self, signal: np.ndarray, method: str = 'savgol',
                     window_length: int = 51, polyorder: int = 3) -> np.ndarray:
        """ä¿¡å·å¹³æ»‘"""
        if method == 'savgol':
            # ç¡®ä¿çª—å£é•¿åº¦ä¸ºå¥‡æ•°ä¸”å°äºä¿¡å·é•¿åº¦
            window_length = min(window_length, len(signal))
            if window_length % 2 == 0:
                window_length -= 1
            if window_length < polyorder + 1:
                window_length = polyorder + 1
                if window_length % 2 == 0:
                    window_length += 1
            
            smoothed = savgol_filter(signal, window_length, polyorder)
            
        elif method == 'moving_average':
            smoothed = np.convolve(signal, np.ones(window_length)/window_length, mode='same')
            
        else:
            raise ValueError(f"Unknown smoothing method: {method}")
        
        return smoothed
    
    def normalize_signal(self, signal: np.ndarray, method: str = 'zscore') -> Tuple[np.ndarray, dict]:
        """ä¿¡å·æ ‡å‡†åŒ–"""
        if method == 'zscore':
            mean_val = np.mean(signal)
            std_val = np.std(signal)
            normalized = (signal - mean_val) / std_val if std_val > 0 else signal - mean_val
            params = {'mean': mean_val, 'std': std_val}
            
        elif method == 'minmax':
            min_val = np.min(signal)
            max_val = np.max(signal)
            range_val = max_val - min_val
            normalized = (signal - min_val) / range_val if range_val > 0 else signal - min_val
            params = {'min': min_val, 'max': max_val}
            
        elif method == 'robust':
            median_val = np.median(signal)
            mad = np.median(np.abs(signal - median_val))
            normalized = (signal - median_val) / mad if mad > 0 else signal - median_val
            params = {'median': median_val, 'mad': mad}
            
        else:
            raise ValueError(f"Unknown normalization method: {method}")
        
        return normalized, params
    
    def segment_signal(self, signal: np.ndarray, segment_length: int,
                      overlap: float = 0.5) -> List[np.ndarray]:
        """ä¿¡å·åˆ†æ®µ"""
        step = int(segment_length * (1 - overlap))
        segments = []
        
        for start in range(0, len(signal) - segment_length + 1, step):
            segment = signal[start:start + segment_length]
            segments.append(segment)
        
        return segments
    
    def preprocess_signal(self, signal: np.ndarray, 
                         remove_outliers: bool = True,
                         apply_filter: bool = True,
                         smooth: bool = False,
                         normalize: bool = True,
                         **kwargs) -> Tuple[np.ndarray, dict]:
        """å®Œæ•´çš„ä¿¡å·é¢„å¤„ç†æµç¨‹"""
        processed_signal = signal.copy()
        processing_info = {'original_length': len(signal)}
        
        # 1. ç§»é™¤å¼‚å¸¸å€¼
        if remove_outliers:
            outlier_method = kwargs.get('outlier_method', 'iqr')
            processed_signal = self.remove_outliers(processed_signal, method=outlier_method)
            processing_info['outlier_removal'] = outlier_method
        
        # 2. æ»¤æ³¢
        if apply_filter:
            filter_type = kwargs.get('filter_type', 'lowpass')
            cutoff = kwargs.get('cutoff', 1000)
            processed_signal = self.apply_filter(processed_signal, filter_type, cutoff)
            processing_info['filter'] = {'type': filter_type, 'cutoff': cutoff}
        
        # 3. å¹³æ»‘
        if smooth:
            smooth_method = kwargs.get('smooth_method', 'savgol')
            window_length = kwargs.get('window_length', 51)
            processed_signal = self.smooth_signal(processed_signal, smooth_method, window_length)
            processing_info['smoothing'] = {'method': smooth_method, 'window': window_length}
        
        # 4. æ ‡å‡†åŒ–
        if normalize:
            norm_method = kwargs.get('norm_method', 'zscore')
            processed_signal, norm_params = self.normalize_signal(processed_signal, norm_method)
            processing_info['normalization'] = {'method': norm_method, 'params': norm_params}
        
        processing_info['final_length'] = len(processed_signal)
        
        return processed_signal, processing_info
    
    def preprocess_dataset(self, data: Dict, **preprocessing_kwargs) -> Tuple[Dict, Dict]:
        """é¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
        print("ğŸ”§ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        
        processed_data = {}
        processing_logs = {}
        
        for sensor in data.keys():
            processed_data[sensor] = {}
            processing_logs[sensor] = {}
            
            for state in data[sensor].keys():
                signals = data[sensor][state]
                processed_signals = []
                state_logs = []
                
                print(f"  å¤„ç† {sensor}/{state}: {len(signals)} ä¸ªä¿¡å·")
                
                for i, signal in enumerate(signals):
                    try:
                        processed_signal, info = self.preprocess_signal(signal, **preprocessing_kwargs)
                        processed_signals.append(processed_signal)
                        state_logs.append(info)
                        
                    except Exception as e:
                        print(f"    âŒ å¤„ç†å¤±è´¥ {sensor}/{state} ä¿¡å· {i}: {e}")
                        continue
                
                processed_data[sensor][state] = processed_signals
                processing_logs[sensor][state] = state_logs
                
                print(f"    âœ… å®Œæˆ {len(processed_signals)}/{len(signals)} ä¸ªä¿¡å·")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        self._save_processing_logs(processing_logs)
        
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        return processed_data, processing_logs
    
    def _save_processing_logs(self, logs: Dict):
        """ä¿å­˜é¢„å¤„ç†æ—¥å¿—"""
        import json
        from datetime import datetime
        
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'processing_logs': logs
        }
        
        log_path = self.output_path / 'preprocessing_logs.json'
        with open(log_path, 'w') as f:
            json.dump(log_data, f, indent=2, default=str)
        
        print(f"ğŸ“‹ é¢„å¤„ç†æ—¥å¿—å·²ä¿å­˜: {log_path}")
    
    def create_train_test_split(self, data: Dict, test_ratio: float = 0.2,
                               val_ratio: float = 0.1, random_state: int = 42) -> Dict:
        """åˆ›å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†"""
        print("ğŸ“Š åˆ›å»ºæ•°æ®é›†åˆ’åˆ†...")
        
        np.random.seed(random_state)
        splits = {'train': {}, 'val': {}, 'test': {}}
        
        for sensor in data.keys():
            for split in splits.keys():
                splits[split][sensor] = {}
        
        split_info = []
        
        for sensor in data.keys():
            for state in data[sensor].keys():
                signals = data[sensor][state]
                n_signals = len(signals)
                
                if n_signals == 0:
                    continue
                
                # éšæœºæ‰“ä¹±ç´¢å¼•
                indices = np.random.permutation(n_signals)
                
                # è®¡ç®—åˆ’åˆ†ç‚¹
                n_test = max(1, int(n_signals * test_ratio))
                n_val = max(1, int(n_signals * val_ratio))
                n_train = n_signals - n_test - n_val
                
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬åœ¨è®­ç»ƒé›†
                if n_train <= 0:
                    n_train = 1
                    n_val = max(0, n_signals - n_train - n_test)
                    n_test = n_signals - n_train - n_val
                
                # åˆ’åˆ†æ•°æ®
                train_indices = indices[:n_train]
                val_indices = indices[n_train:n_train + n_val]
                test_indices = indices[n_train + n_val:]
                
                splits['train'][sensor][state] = [signals[i] for i in train_indices]
                splits['val'][sensor][state] = [signals[i] for i in val_indices]
                splits['test'][sensor][state] = [signals[i] for i in test_indices]
                
                # è®°å½•åˆ’åˆ†ä¿¡æ¯
                split_info.append({
                    'sensor': sensor,
                    'state': state,
                    'total': n_signals,
                    'train': len(train_indices),
                    'val': len(val_indices),
                    'test': len(test_indices)
                })
        
        # ä¿å­˜åˆ’åˆ†ä¿¡æ¯
        split_df = pd.DataFrame(split_info)
        split_path = self.output_path / 'data_splits.csv'
        split_df.to_csv(split_path, index=False)
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ†ä¿¡æ¯å·²ä¿å­˜: {split_path}")
        print("ğŸ“‹ åˆ’åˆ†ç»Ÿè®¡:")
        print(split_df.groupby('state')[['train', 'val', 'test']].sum())
        
        return splits

if __name__ == "__main__":
    # æµ‹è¯•é¢„å¤„ç†å™¨
    from data_loader import MotorDataLoader
    
    # åŠ è½½æ•°æ®
    loader = MotorDataLoader("../../experiments/configs/config.yaml")
    data, _ = loader.load_all_data(max_files_per_state=10)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = SignalPreprocessor("../../experiments/configs/config.yaml")
    
    # é¢„å¤„ç†æ•°æ®
    processed_data, logs = preprocessor.preprocess_dataset(
        data,
        remove_outliers=True,
        apply_filter=True,
        normalize=True,
        filter_type='lowpass',
        cutoff=1000
    )
    
    # åˆ›å»ºæ•°æ®é›†åˆ’åˆ†
    splits = preprocessor.create_train_test_split(processed_data)
    
    print("\nğŸ‰ é¢„å¤„ç†æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {preprocessor.output_path}")
