#!/usr/bin/env python3
"""
å¢å¼ºæ•°æ®åŠ è½½å™¨
å……åˆ†åˆ©ç”¨å¤šä¼ æ„Ÿå™¨æ•°æ®ï¼Œå®ç°æ•°æ®èåˆå’Œå¢å¼º
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import yaml
from typing import Dict, List, Tuple, Optional
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

class EnhancedMotorDataLoader:
    """å¢å¼ºçš„ç”µæœºæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.data_path = Path(self.config['data']['path'])
        self.output_path = Path(self.config['output']['tables'])
        
        # ä¼ æ„Ÿå™¨é…ç½®
        self.sensors = ['ShengYing', 'ZhenDong']  # å£°éŸ³å’ŒæŒ¯åŠ¨ä¼ æ„Ÿå™¨
        self.states = ['normal', 'spark', 'vibrate']
        
        print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {self.data_path}")
        print(f"ğŸ”§ ä¼ æ„Ÿå™¨: {self.sensors}")
        print(f"ğŸ“Š çŠ¶æ€: {self.states}")
    
    def load_all_data_enhanced(self, max_files_per_state: int = None) -> Tuple[Dict, Dict]:
        """å¢å¼ºæ•°æ®åŠ è½½ - å……åˆ†åˆ©ç”¨æ‰€æœ‰æ•°æ®"""
        print("ğŸ“‚ å¼€å§‹å¢å¼ºæ•°æ®åŠ è½½...")
        
        all_data = {}
        file_info = {}
        
        for sensor in self.sensors:
            all_data[sensor] = {}
            file_info[sensor] = {}
            
            sensor_path = self.data_path / sensor
            
            for state in self.states:
                state_path = sensor_path / state
                
                if not state_path.exists():
                    print(f"âš ï¸ è·¯å¾„ä¸å­˜åœ¨: {state_path}")
                    all_data[sensor][state] = []
                    file_info[sensor][state] = []
                    continue
                
                # è·å–æ‰€æœ‰.csvæ–‡ä»¶
                csv_files = list(state_path.glob("*.csv"))

                if max_files_per_state:
                    csv_files = csv_files[:max_files_per_state]

                signals = []
                files = []

                for file_path in csv_files:
                    try:
                        # è¯»å–CSVä¿¡å·æ•°æ®
                        data = pd.read_csv(file_path, header=None)

                        # å‡è®¾ä¿¡å·åœ¨ç¬¬ä¸€åˆ—ï¼Œå¦‚æœæœ‰å¤šåˆ—å–ç¬¬ä¸€åˆ—
                        if len(data.columns) > 1:
                            signal = data.iloc[:, 0].values  # å–ç¬¬ä¸€åˆ—
                        else:
                            signal = data.values.flatten()

                        # åŸºæœ¬è´¨é‡æ£€æŸ¥
                        if len(signal) > 100 and not np.any(np.isnan(signal)):
                            signals.append(signal)
                            files.append(file_path.name)

                    except Exception as e:
                        print(f"âŒ è¯»å–å¤±è´¥ {file_path}: {e}")
                        continue
                
                all_data[sensor][state] = signals
                file_info[sensor][state] = files
                
                print(f"âœ… åŠ è½½ {sensor}/{state}: {len(signals)} ä¸ªæ–‡ä»¶")
        
        return all_data, file_info
    
    def create_multi_sensor_dataset(self, data: Dict, file_info: Dict) -> Tuple[Dict, Dict]:
        """åˆ›å»ºå¤šä¼ æ„Ÿå™¨èåˆæ•°æ®é›†"""
        print("ğŸ”— åˆ›å»ºå¤šä¼ æ„Ÿå™¨èåˆæ•°æ®é›†...")

        # æŒ‰æ–‡ä»¶ååŒ¹é…å¤šä¼ æ„Ÿå™¨æ•°æ®
        matched_data = {}
        single_sensor_data = {}

        for state in self.states:
            matched_data[state] = []
            single_sensor_data[state] = []

            # è·å–ä¸¤ä¸ªä¼ æ„Ÿå™¨çš„æ–‡ä»¶å
            shengying_files = set(f.replace('.csv', '') for f in
                                file_info.get('ShengYing', {}).get(state, []))
            zhendong_files = set(f.replace('.csv', '') for f in
                               file_info.get('ZhenDong', {}).get(state, []))
            
            # æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶
            common_files = shengying_files.intersection(zhendong_files)
            
            print(f"  {state}: å£°éŸ³{len(shengying_files)}, æŒ¯åŠ¨{len(zhendong_files)}, åŒ¹é…{len(common_files)}")
            
            # åˆ›å»ºåŒ¹é…çš„å¤šä¼ æ„Ÿå™¨æ ·æœ¬
            for file_base in common_files:
                try:
                    # æ‰¾åˆ°å¯¹åº”çš„ä¿¡å·
                    sy_idx = next(i for i, f in enumerate(file_info['ShengYing'][state])
                                if f.replace('.csv', '') == file_base)
                    zd_idx = next(i for i, f in enumerate(file_info['ZhenDong'][state])
                                if f.replace('.csv', '') == file_base)
                    
                    sy_signal = data['ShengYing'][state][sy_idx]
                    zd_signal = data['ZhenDong'][state][zd_idx]
                    
                    # å¤šä¼ æ„Ÿå™¨èåˆæ ·æœ¬
                    matched_data[state].append({
                        'ShengYing': sy_signal,
                        'ZhenDong': zd_signal,
                        'file_id': file_base
                    })
                    
                except:
                    continue
            
            # æ·»åŠ å•ä¼ æ„Ÿå™¨æ ·æœ¬
            for sensor in self.sensors:
                if sensor in data and state in data[sensor]:
                    for i, signal in enumerate(data[sensor][state]):
                        single_sensor_data[state].append({
                            'sensor': sensor,
                            'signal': signal,
                            'file_id': f"{sensor}_{state}_{i}"
                        })
        
        return matched_data, single_sensor_data
    
    def augment_data(self, signals: List[np.ndarray], augment_factor: int = 3) -> List[np.ndarray]:
        """æ•°æ®å¢å¼º"""
        augmented_signals = []
        
        for signal in signals:
            # åŸå§‹ä¿¡å·
            augmented_signals.append(signal)
            
            for _ in range(augment_factor):
                # æ·»åŠ å™ªå£°
                noise_level = 0.01 * np.std(signal)
                noisy_signal = signal + np.random.normal(0, noise_level, len(signal))
                augmented_signals.append(noisy_signal)
                
                # æ—¶é—´æ‹‰ä¼¸/å‹ç¼©
                stretch_factor = np.random.uniform(0.9, 1.1)
                stretched_indices = np.linspace(0, len(signal)-1, 
                                              int(len(signal) * stretch_factor))
                stretched_signal = np.interp(stretched_indices, 
                                           np.arange(len(signal)), signal)
                # é‡é‡‡æ ·åˆ°åŸé•¿åº¦
                if len(stretched_signal) != len(signal):
                    stretched_signal = np.interp(np.linspace(0, len(stretched_signal)-1, len(signal)),
                                                np.arange(len(stretched_signal)), stretched_signal)
                augmented_signals.append(stretched_signal)
                
                # å¹…å€¼ç¼©æ”¾
                scale_factor = np.random.uniform(0.8, 1.2)
                scaled_signal = signal * scale_factor
                augmented_signals.append(scaled_signal)
        
        print(f"ğŸ“ˆ æ•°æ®å¢å¼º: {len(signals)} -> {len(augmented_signals)} ä¸ªæ ·æœ¬")
        return augmented_signals
    
    def create_fusion_features(self, matched_data: Dict) -> Tuple[List, List]:
        """åˆ›å»ºèåˆç‰¹å¾"""
        print("ğŸ”— åˆ›å»ºå¤šä¼ æ„Ÿå™¨èåˆç‰¹å¾...")
        
        fusion_signals = []
        fusion_labels = []
        
        for state in self.states:
            for sample in matched_data[state]:
                sy_signal = sample['ShengYing']
                zd_signal = sample['ZhenDong']
                
                # ç¡®ä¿ä¿¡å·é•¿åº¦ä¸€è‡´
                min_len = min(len(sy_signal), len(zd_signal))
                sy_signal = sy_signal[:min_len]
                zd_signal = zd_signal[:min_len]
                
                # å¤šç§èåˆç­–ç•¥
                fusion_methods = [
                    # 1. ç®€å•æ‹¼æ¥
                    np.concatenate([sy_signal, zd_signal]),
                    
                    # 2. åŠ æƒå¹³å‡
                    0.6 * sy_signal + 0.4 * zd_signal,
                    
                    # 3. å·®å€¼ç‰¹å¾
                    sy_signal - zd_signal,
                    
                    # 4. ä¹˜ç§¯ç‰¹å¾
                    sy_signal * zd_signal,
                    
                    # 5. æœ€å¤§å€¼ç‰¹å¾
                    np.maximum(sy_signal, zd_signal),
                ]
                
                for fused_signal in fusion_methods:
                    fusion_signals.append(fused_signal)
                    fusion_labels.append(state)
        
        print(f"ğŸ”— èåˆç‰¹å¾åˆ›å»ºå®Œæˆ: {len(fusion_signals)} ä¸ªæ ·æœ¬")
        return fusion_signals, fusion_labels
    
    def load_comprehensive_dataset(self, max_files_per_state: int = None, 
                                 enable_augmentation: bool = True) -> Tuple[Dict, Dict]:
        """åŠ è½½ç»¼åˆæ•°æ®é›†"""
        print("ğŸš€ å¼€å§‹åŠ è½½ç»¼åˆæ•°æ®é›†...")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        raw_data, file_info = self.load_all_data_enhanced(max_files_per_state)
        
        # 2. åˆ›å»ºå¤šä¼ æ„Ÿå™¨æ•°æ®é›†
        matched_data, single_sensor_data = self.create_multi_sensor_dataset(raw_data, file_info)
        
        # 3. åˆ›å»ºæœ€ç»ˆæ•°æ®é›†
        final_dataset = {
            'single_sensor': {},
            'multi_sensor': {},
            'fusion_features': {}
        }
        
        # å•ä¼ æ„Ÿå™¨æ•°æ®
        for state in self.states:
            final_dataset['single_sensor'][state] = []
            
            # æ”¶é›†æ‰€æœ‰å•ä¼ æ„Ÿå™¨ä¿¡å·
            all_signals = []
            for sensor in self.sensors:
                if sensor in raw_data and state in raw_data[sensor]:
                    all_signals.extend(raw_data[sensor][state])
            
            # æ•°æ®å¢å¼º
            if enable_augmentation and len(all_signals) > 0:
                all_signals = self.augment_data(all_signals, augment_factor=2)
            
            final_dataset['single_sensor'][state] = all_signals
        
        # å¤šä¼ æ„Ÿå™¨åŒ¹é…æ•°æ®
        for state in self.states:
            final_dataset['multi_sensor'][state] = []
            for sample in matched_data[state]:
                # æ·»åŠ åŸå§‹å¤šä¼ æ„Ÿå™¨æ ·æœ¬
                final_dataset['multi_sensor'][state].append({
                    'ShengYing': sample['ShengYing'],
                    'ZhenDong': sample['ZhenDong']
                })
        
        # èåˆç‰¹å¾
        fusion_signals, fusion_labels = self.create_fusion_features(matched_data)
        for state in self.states:
            final_dataset['fusion_features'][state] = [
                sig for sig, label in zip(fusion_signals, fusion_labels) if label == state
            ]
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š ç»¼åˆæ•°æ®é›†ç»Ÿè®¡:")
        for dataset_type in final_dataset:
            print(f"  {dataset_type}:")
            for state in self.states:
                count = len(final_dataset[dataset_type][state])
                print(f"    {state}: {count} ä¸ªæ ·æœ¬")
        
        return final_dataset, file_info
    
    def save_dataset_info(self, dataset: Dict, file_info: Dict):
        """ä¿å­˜æ•°æ®é›†ä¿¡æ¯"""
        # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯
        stats = {}
        total_samples = 0
        
        for dataset_type in dataset:
            stats[dataset_type] = {}
            for state in self.states:
                count = len(dataset[dataset_type][state])
                stats[dataset_type][state] = count
                total_samples += count
        
        stats['total_samples'] = total_samples
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        import json
        from datetime import datetime
        
        info_data = {
            'timestamp': datetime.now().isoformat(),
            'statistics': stats,
            'file_info': file_info
        }
        
        info_path = self.output_path / 'enhanced_dataset_info.json'
        with open(info_path, 'w') as f:
            json.dump(info_data, f, indent=2, default=str)
        
        print(f"ğŸ“‹ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜: {info_path}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")

if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºæ•°æ®åŠ è½½å™¨
    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
    
    # åˆ›å»ºåŠ è½½å™¨
    loader = EnhancedMotorDataLoader(str(config_path))
    
    # åŠ è½½ç»¼åˆæ•°æ®é›†
    dataset, file_info = loader.load_comprehensive_dataset(
        max_files_per_state=None,  # ä½¿ç”¨æ‰€æœ‰æ•°æ®
        enable_augmentation=True
    )
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
    loader.save_dataset_info(dataset, file_info)
    
    print("\nğŸ‰ å¢å¼ºæ•°æ®åŠ è½½å®Œæˆï¼")
