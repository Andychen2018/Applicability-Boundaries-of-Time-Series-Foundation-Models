#!/usr/bin/env python3
"""
ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹æ¨¡å—
åŒ…å«å¤šç§ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•çš„è®­ç»ƒå’Œè¯„ä¼°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve)
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
import xgboost as xgb
import lightgbm as lgb
from typing import Dict, List, Tuple, Optional
import yaml
from pathlib import Path
import joblib
import json
from datetime import datetime

class TraditionalMLPipeline:
    """ä¼ ç»Ÿæœºå™¨å­¦ä¹ æµæ°´çº¿"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.image_path = Path(self.config['output']['images'])
        self.models_path = self.output_path.parent / 'models'
        self.models_path.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.models = {
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),
            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'SVM': SVC(random_state=42, probability=True),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'Naive Bayes': GaussianNB(),
            'Decision Tree': DecisionTreeClassifier(random_state=42)
        }
        
        self.results = {}
        self.trained_models = {}
        
    def load_features(self, features_path: str) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        print("ğŸ“Š åŠ è½½ç‰¹å¾æ•°æ®...")
        
        features_df = pd.read_csv(features_path)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in features_df.columns 
                       if col not in ['label', 'sensor', 'file_id']]
        
        X = features_df[feature_cols]
        y = features_df['label']
        
        # å¤„ç†ç¼ºå¤±å€¼å’Œæ— ç©·å€¼
        X = X.replace([np.inf, -np.inf], np.nan)
        X = X.fillna(X.median())
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(X)} ä¸ªæ ·æœ¬, {len(feature_cols)} ä¸ªç‰¹å¾")
        print(f"ğŸ“‹ ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts())}")
        
        return features_df, X.values, y.values
    
    def preprocess_data(self, X: np.ndarray, y: np.ndarray, 
                       test_size: float = 0.2, val_size: float = 0.1) -> Dict:
        """æ•°æ®é¢„å¤„ç†å’Œåˆ’åˆ†"""
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
        
        # ç¼–ç æ ‡ç­¾
        self.label_encoder = LabelEncoder()
        y_encoded = self.label_encoder.fit_transform(y)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded
        )
        
        # ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†
        val_size_adjusted = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        data_splits = {
            'X_train': X_train_scaled,
            'X_val': X_val_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_val': y_val,
            'y_test': y_test,
            'X_train_raw': X_train,
            'X_val_raw': X_val,
            'X_test_raw': X_test
        }
        
        print(f"âœ… æ•°æ®åˆ’åˆ†å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        return data_splits
    
    def train_models(self, data_splits: Dict) -> Dict:
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        X_train = data_splits['X_train']
        y_train = data_splits['y_train']
        X_val = data_splits['X_val']
        y_val = data_splits['y_val']
        
        for name, model in self.models.items():
            print(f"  è®­ç»ƒ {name}...")
            
            try:
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)
                self.trained_models[name] = model
                
                # éªŒè¯é›†é¢„æµ‹
                y_val_pred = model.predict(X_val)
                y_val_prob = model.predict_proba(X_val) if hasattr(model, 'predict_proba') else None
                
                # è®¡ç®—æŒ‡æ ‡
                metrics = self._calculate_metrics(y_val, y_val_pred, y_val_prob)
                self.results[name] = {
                    'model': model,
                    'val_metrics': metrics,
                    'val_predictions': y_val_pred,
                    'val_probabilities': y_val_prob
                }
                
                print(f"    âœ… {name} - éªŒè¯å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
                
            except Exception as e:
                print(f"    âŒ {name} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.results
    
    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                          y_prob: Optional[np.ndarray] = None) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted'),
            'recall': recall_score(y_true, y_pred, average='weighted'),
            'f1': f1_score(y_true, y_pred, average='weighted')
        }
        
        # å¤šç±»åˆ«AUC
        if y_prob is not None:
            try:
                metrics['auc'] = roc_auc_score(y_true, y_prob, multi_class='ovr', average='weighted')
            except:
                metrics['auc'] = 0.0
        else:
            metrics['auc'] = 0.0
        
        return metrics
    
    def evaluate_on_test_set(self, data_splits: Dict) -> Dict:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        print("ğŸ“Š æµ‹è¯•é›†è¯„ä¼°...")
        
        X_test = data_splits['X_test']
        y_test = data_splits['y_test']
        
        test_results = {}
        
        for name, result in self.results.items():
            model = result['model']
            
            # æµ‹è¯•é›†é¢„æµ‹
            y_test_pred = model.predict(X_test)
            y_test_prob = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
            
            # è®¡ç®—æŒ‡æ ‡
            test_metrics = self._calculate_metrics(y_test, y_test_pred, y_test_prob)
            
            test_results[name] = {
                'test_metrics': test_metrics,
                'test_predictions': y_test_pred,
                'test_probabilities': y_test_prob
            }
            
            print(f"  {name} - æµ‹è¯•å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")
        
        return test_results
    
    def hyperparameter_tuning(self, data_splits: Dict, model_names: List[str] = None) -> Dict:
        """è¶…å‚æ•°è°ƒä¼˜"""
        print("ğŸ”§ è¶…å‚æ•°è°ƒä¼˜...")
        
        if model_names is None:
            model_names = ['Random Forest', 'XGBoost', 'SVM']
        
        X_train = data_splits['X_train']
        y_train = data_splits['y_train']
        
        # å®šä¹‰å‚æ•°ç½‘æ ¼
        param_grids = {
            'Random Forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10]
            },
            'XGBoost': {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 10],
                'learning_rate': [0.01, 0.1, 0.2]
            },
            'SVM': {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto'],
                'kernel': ['rbf', 'linear']
            }
        }
        
        tuned_models = {}
        
        for name in model_names:
            if name in param_grids:
                print(f"  è°ƒä¼˜ {name}...")
                
                base_model = self.models[name]
                param_grid = param_grids[name]
                
                # ç½‘æ ¼æœç´¢
                grid_search = GridSearchCV(
                    base_model, param_grid, cv=3, scoring='accuracy',
                    n_jobs=-1, verbose=0
                )
                
                grid_search.fit(X_train, y_train)
                
                tuned_models[name] = {
                    'best_model': grid_search.best_estimator_,
                    'best_params': grid_search.best_params_,
                    'best_score': grid_search.best_score_
                }
                
                print(f"    âœ… {name} æœ€ä½³åˆ†æ•°: {grid_search.best_score_:.4f}")
                print(f"    ğŸ“‹ æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        
        return tuned_models
    
    def save_models(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        for name, model in self.trained_models.items():
            model_path = self.models_path / f"{name.replace(' ', '_').lower()}_model.pkl"
            joblib.dump(model, model_path)
            print(f"  âœ… {name} å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = self.models_path / "scaler.pkl"
        joblib.dump(self.scaler, scaler_path)
        
        encoder_path = self.models_path / "label_encoder.pkl"
        joblib.dump(self.label_encoder, encoder_path)
        
        print(f"âœ… é¢„å¤„ç†å™¨å·²ä¿å­˜")
    
    def save_results(self, test_results: Dict):
        """ä¿å­˜å®éªŒç»“æœ"""
        print("ğŸ“‹ ä¿å­˜å®éªŒç»“æœ...")
        
        # æ•´ç†ç»“æœæ•°æ®
        results_data = []
        
        for name in self.results.keys():
            val_metrics = self.results[name]['val_metrics']
            test_metrics = test_results[name]['test_metrics']
            
            result_row = {
                'model': name,
                'val_accuracy': val_metrics['accuracy'],
                'val_precision': val_metrics['precision'],
                'val_recall': val_metrics['recall'],
                'val_f1': val_metrics['f1'],
                'val_auc': val_metrics['auc'],
                'test_accuracy': test_metrics['accuracy'],
                'test_precision': test_metrics['precision'],
                'test_recall': test_metrics['recall'],
                'test_f1': test_metrics['f1'],
                'test_auc': test_metrics['auc']
            }
            results_data.append(result_row)
        
        # ä¿å­˜ä¸ºCSV
        results_df = pd.DataFrame(results_data)
        results_path = self.output_path / 'traditional_ml_results.csv'
        results_df.to_csv(results_path, index=False)
        
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜: {results_path}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœä¸ºJSON
        detailed_results = {
            'timestamp': datetime.now().isoformat(),
            'results': results_data,
            'class_names': self.label_encoder.classes_.tolist()
        }
        
        json_path = self.output_path / 'traditional_ml_detailed_results.json'
        with open(json_path, 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        return results_df

if __name__ == "__main__":
    # æµ‹è¯•ä¼ ç»Ÿæœºå™¨å­¦ä¹ æµæ°´çº¿
    from pathlib import Path
    
    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
    features_path = Path(__file__).parent.parent.parent / "output/table/extracted_features.csv"
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = TraditionalMLPipeline(str(config_path))
    
    # åŠ è½½ç‰¹å¾
    features_df, X, y = pipeline.load_features(str(features_path))
    
    # æ•°æ®é¢„å¤„ç†
    data_splits = pipeline.preprocess_data(X, y)
    
    # è®­ç»ƒæ¨¡å‹
    results = pipeline.train_models(data_splits)
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_results = pipeline.evaluate_on_test_set(data_splits)
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    pipeline.save_models()
    results_df = pipeline.save_results(test_results)
    
    print("\nğŸ‰ ä¼ ç»Ÿæœºå™¨å­¦ä¹ å®éªŒå®Œæˆï¼")
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰æµ‹è¯•F1åˆ†æ•°):")
    print(results_df.sort_values('test_f1', ascending=False)[['model', 'test_accuracy', 'test_f1']].to_string(index=False))
