#!/usr/bin/env python3
"""
æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨¡å—
åŒ…å«1D-CNN, LSTM, Transformerç­‰æ·±åº¦å­¦ä¹ æ–¹æ³•
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
import matplotlib.pyplot as plt
from pathlib import Path
import yaml
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class MotorSignalDataset(Dataset):
    """ç”µæœºä¿¡å·æ•°æ®é›†"""
    
    def __init__(self, signals: List[np.ndarray], labels: np.ndarray, max_length: int = 5000):
        self.signals = signals
        self.labels = labels
        self.max_length = max_length
        
    def __len__(self):
        return len(self.signals)
    
    def __getitem__(self, idx):
        signal = self.signals[idx]
        label = self.labels[idx]
        
        # æˆªæ–­æˆ–å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(signal) > self.max_length:
            signal = signal[:self.max_length]
        else:
            signal = np.pad(signal, (0, self.max_length - len(signal)), 'constant')
        
        return torch.FloatTensor(signal).unsqueeze(0), torch.LongTensor([label])

class CNN1D(nn.Module):
    """1Då·ç§¯ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_length: int = 5000, num_classes: int = 3):
        super(CNN1D, self).__init__()
        
        self.conv_layers = nn.Sequential(
            # ç¬¬ä¸€å±‚å·ç§¯ - å‡å°å·ç§¯æ ¸
            nn.Conv1d(1, 32, kernel_size=16, stride=2, padding=8),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.MaxPool1d(4),
            nn.Dropout(0.2),

            # ç¬¬äºŒå±‚å·ç§¯
            nn.Conv1d(32, 64, kernel_size=8, stride=2, padding=4),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(4),
            nn.Dropout(0.2),

            # ç¬¬ä¸‰å±‚å·ç§¯
            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=2),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(4),
            nn.Dropout(0.2),

            # ç¬¬å››å±‚å·ç§¯
            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=2),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        x = self.conv_layers(x)
        x = self.classifier(x)
        return x

class LSTMModel(nn.Module):
    """LSTMæ¨¡å‹ - ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""

    def __init__(self, input_size: int = 1, hidden_size: int = 64,
                 num_layers: int = 2, num_classes: int = 3):
        super(LSTMModel, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # å‡å°éšè—å±‚å¤§å°ä»¥èŠ‚çœå†…å­˜
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           batch_first=True, dropout=0.2, bidirectional=False)

        # ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )
    
    def forward(self, x):
        # x shape: (batch_size, 1, seq_len) -> (batch_size, seq_len, 1)
        x = x.transpose(1, 2)

        # LSTM
        lstm_out, _ = self.lstm(x)

        # Self-attention
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # Global average pooling
        pooled = torch.mean(attn_out, dim=1)

        # Classification
        output = self.classifier(pooled)
        return output

class TransformerModel(nn.Module):
    """Transformeræ¨¡å‹"""
    
    def __init__(self, input_size: int = 1, d_model: int = 128, 
                 nhead: int = 8, num_layers: int = 4, num_classes: int = 3):
        super(TransformerModel, self).__init__()
        
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = nn.Parameter(torch.randn(5000, d_model))
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=512,
            dropout=0.1, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        # x shape: (batch_size, 1, seq_len) -> (batch_size, seq_len, 1)
        x = x.transpose(1, 2)
        seq_len = x.size(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:seq_len, :].unsqueeze(0)
        
        # Transformerç¼–ç 
        x = self.transformer(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(x, dim=1)
        
        # åˆ†ç±»
        output = self.classifier(x)
        return output

class AutoEncoder(nn.Module):
    """è‡ªç¼–ç å™¨ç”¨äºä¿¡å·é‡å»º"""
    
    def __init__(self, input_length: int = 5000, encoding_dim: int = 128):
        super(AutoEncoder, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=64, stride=4),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=32, stride=4),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=16, stride=4),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(encoding_dim)
        )
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(128, 64, kernel_size=16, stride=4),
            nn.ReLU(),
            nn.ConvTranspose1d(64, 32, kernel_size=32, stride=4),
            nn.ReLU(),
            nn.ConvTranspose1d(32, 1, kernel_size=64, stride=4),
        )
        
        # åˆ†ç±»å™¨ï¼ˆåŸºäºç¼–ç ç‰¹å¾ï¼‰
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(encoding_dim * 128, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 3)
        )
    
    def forward(self, x, return_reconstruction=False):
        # ç¼–ç 
        encoded = self.encoder(x)
        
        if return_reconstruction:
            # è§£ç é‡å»º
            decoded = self.decoder(encoded)
            # è°ƒæ•´è¾“å‡ºé•¿åº¦
            if decoded.size(-1) != x.size(-1):
                decoded = nn.functional.interpolate(decoded, size=x.size(-1), mode='linear', align_corners=False)
            return decoded, encoded
        else:
            # åˆ†ç±»
            classification = self.classifier(encoded)
            return classification

class DeepLearningPipeline:
    """æ·±åº¦å­¦ä¹ æµæ°´çº¿"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.image_path = Path(self.config['output']['images'])
        self.models_path = self.output_path.parent / 'models'
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.models = {
            'CNN1D': CNN1D,
            'LSTM': LSTMModel,
            'Transformer': TransformerModel,
            'AutoEncoder': AutoEncoder
        }
        
        self.results = {}
    
    def load_data(self) -> Tuple[List[np.ndarray], np.ndarray]:
        """åŠ è½½åŸå§‹ä¿¡å·æ•°æ®"""
        print("ğŸ“‚ åŠ è½½åŸå§‹ä¿¡å·æ•°æ®...")
        
        # ä»data_processingæ¨¡å—åŠ è½½æ•°æ®
        import sys
        sys.path.append(str(Path(__file__).parent.parent / 'data_processing'))
        from data_loader import MotorDataLoader
        
        config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
        loader = MotorDataLoader(str(config_path))
        data, _ = loader.load_all_data(max_files_per_state=50)
        
        # æ•´ç†æ•°æ®
        all_signals = []
        all_labels = []
        
        label_map = {'normal': 0, 'spark': 1, 'vibrate': 2}
        
        for sensor in data.keys():
            for state in data[sensor].keys():
                signals = data[sensor][state]
                for signal in signals:
                    all_signals.append(signal)
                    all_labels.append(label_map[state])
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(all_signals)} ä¸ªä¿¡å·")
        return all_signals, np.array(all_labels)
    
    def prepare_data(self, signals: List[np.ndarray], labels: np.ndarray) -> Dict:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # æ•°æ®åˆ’åˆ†
        X_temp, X_test, y_temp, y_test = train_test_split(
            signals, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MotorSignalDataset(X_train, y_train)
        val_dataset = MotorSignalDataset(X_val, y_val)
        test_dataset = MotorSignalDataset(X_test, y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å‡å°batch sizeä»¥èŠ‚çœå†…å­˜
        train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)
        
        data_loaders = {
            'train': train_loader,
            'val': val_loader,
            'test': test_loader
        }
        
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        
        return data_loaders

    def train_model(self, model_name: str, data_loaders: Dict, epochs: int = 50) -> Dict:
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"ğŸ¯ è®­ç»ƒ {model_name}...")

        # åˆ›å»ºæ¨¡å‹
        model = self.models[model_name]().to(self.device)

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_accuracies = []

        best_val_acc = 0.0
        best_model_state = None
        patience_counter = 0

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0

            for batch_signals, batch_labels in data_loaders['train']:
                batch_signals = batch_signals.to(self.device)
                batch_labels = batch_labels.squeeze().to(self.device)

                optimizer.zero_grad()

                if model_name == 'AutoEncoder':
                    outputs = model(batch_signals)
                else:
                    outputs = model(batch_signals)

                loss = criterion(outputs, batch_labels)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            correct = 0
            total = 0

            with torch.no_grad():
                for batch_signals, batch_labels in data_loaders['val']:
                    batch_signals = batch_signals.to(self.device)
                    batch_labels = batch_labels.squeeze().to(self.device)

                    if model_name == 'AutoEncoder':
                        outputs = model(batch_signals)
                    else:
                        outputs = model(batch_signals)

                    loss = criterion(outputs, batch_labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs.data, 1)
                    total += batch_labels.size(0)
                    correct += (predicted == batch_labels).sum().item()

            # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_train_loss = train_loss / len(data_loaders['train'])
            avg_val_loss = val_loss / len(data_loaders['val'])
            val_accuracy = correct / total

            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_accuracy)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)

            # æ—©åœæ£€æŸ¥
            if val_accuracy > best_val_acc:
                best_val_acc = val_accuracy
                best_model_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1

            if epoch % 10 == 0:
                print(f"  Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, "
                      f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

            # æ—©åœ
            if patience_counter >= 10:
                print(f"  æ—©åœäº epoch {epoch}")
                break

        # æ¢å¤æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)

        # ä¿å­˜æ¨¡å‹
        model_path = self.models_path / f'{model_name.lower()}_model.pth'
        torch.save(model.state_dict(), model_path)

        training_history = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies,
            'best_val_acc': best_val_acc,
            'model': model
        }

        print(f"  âœ… {model_name} è®­ç»ƒå®Œæˆ, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

        return training_history

    def evaluate_model(self, model: nn.Module, data_loader: DataLoader, model_name: str) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        all_predictions = []
        all_labels = []
        all_probabilities = []

        with torch.no_grad():
            for batch_signals, batch_labels in data_loader:
                batch_signals = batch_signals.to(self.device)
                batch_labels = batch_labels.squeeze().to(self.device)

                if model_name == 'AutoEncoder':
                    outputs = model(batch_signals)
                else:
                    outputs = model(batch_signals)

                probabilities = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)

                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(batch_labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')

        # è®¡ç®—AUC (å¤šç±»åˆ«)
        try:
            from sklearn.metrics import roc_auc_score
            auc = roc_auc_score(all_labels, all_probabilities, multi_class='ovr', average='weighted')
        except:
            auc = 0.0

        return {
            'accuracy': accuracy,
            'f1': f1,
            'auc': auc,
            'predictions': all_predictions,
            'labels': all_labels,
            'probabilities': all_probabilities
        }

    def train_all_models(self, data_loaders: Dict) -> Dict:
        """è®­ç»ƒæ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("ğŸ¤– å¼€å§‹è®­ç»ƒæ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹...")

        all_results = {}

        for model_name in self.models.keys():
            try:
                # è®­ç»ƒæ¨¡å‹
                training_history = self.train_model(model_name, data_loaders)

                # æµ‹è¯•é›†è¯„ä¼°
                test_metrics = self.evaluate_model(
                    training_history['model'],
                    data_loaders['test'],
                    model_name
                )

                all_results[model_name] = {
                    'training_history': training_history,
                    'test_metrics': test_metrics
                }

                print(f"âœ… {model_name} - æµ‹è¯•å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}")

            except Exception as e:
                print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                continue

        self.results = all_results
        return all_results

    def save_results(self):
        """ä¿å­˜æ·±åº¦å­¦ä¹ ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜æ·±åº¦å­¦ä¹ ç»“æœ...")

        # æ•´ç†ç»“æœæ•°æ®
        results_data = []

        for model_name, result in self.results.items():
            test_metrics = result['test_metrics']

            result_row = {
                'model': model_name,
                'test_accuracy': test_metrics['accuracy'],
                'test_f1': test_metrics['f1'],
                'test_auc': test_metrics['auc'],
                'best_val_acc': result['training_history']['best_val_acc']
            }
            results_data.append(result_row)

        # ä¿å­˜ä¸ºCSV
        results_df = pd.DataFrame(results_data)
        results_path = self.output_path / 'deep_learning_results.csv'
        results_df.to_csv(results_path, index=False)

        print(f"ğŸ“Š æ·±åº¦å­¦ä¹ ç»“æœå·²ä¿å­˜: {results_path}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'timestamp': datetime.now().isoformat(),
            'device': str(self.device),
            'results': results_data
        }

        json_path = self.output_path / 'deep_learning_detailed_results.json'
        with open(json_path, 'w') as f:
            json.dump(detailed_results, f, indent=2)

        return results_df

if __name__ == "__main__":
    # æµ‹è¯•æ·±åº¦å­¦ä¹ æµæ°´çº¿
    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"

    # åˆ›å»ºæµæ°´çº¿
    pipeline = DeepLearningPipeline(str(config_path))

    # åŠ è½½æ•°æ®
    signals, labels = pipeline.load_data()

    # å‡†å¤‡æ•°æ®
    data_loaders = pipeline.prepare_data(signals, labels)

    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    results = pipeline.train_all_models(data_loaders)

    # ä¿å­˜ç»“æœ
    results_df = pipeline.save_results()

    print("\nğŸ‰ æ·±åº¦å­¦ä¹ å®éªŒå®Œæˆï¼")
    print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰æµ‹è¯•F1åˆ†æ•°):")
    print(results_df.sort_values('test_f1', ascending=False)[['model', 'test_accuracy', 'test_f1']].to_string(index=False))
