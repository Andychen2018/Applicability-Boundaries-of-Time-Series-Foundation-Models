#!/usr/bin/env python3
"""
MomentFM + Chronos ç»„åˆå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ
å®ç°é¢„æµ‹æ€§å¼‚å¸¸æ£€æµ‹æ–¹æ¡ˆ
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from pathlib import Path
import yaml
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# Chronos imports
try:
    from chronos import ChronosPipeline
    CHRONOS_AVAILABLE = True
    print("âœ… Chronos å¯ç”¨")
except ImportError:
    CHRONOS_AVAILABLE = False
    print("âŒ Chronos ä¸å¯ç”¨")

# MomentFM imports
try:
    from momentfm import MOMENTPipeline
    MOMENT_AVAILABLE = True
    print("âœ… MomentFM å¯ç”¨")
except ImportError:
    MOMENT_AVAILABLE = False
    print("âŒ MomentFM ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ")

class MomentChronosAnomalyDetector:
    """MomentFM + Chronos å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.models_path = self.output_path.parent / 'models'
        self.models_path.mkdir(exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.chronos_pipeline = None
        self.moment_pipeline = None
        
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸš€ åˆå§‹åŒ–åŸºç¡€æ¨¡å‹...")
        
        # åˆå§‹åŒ–Chronos
        if CHRONOS_AVAILABLE:
            try:
                print("  åŠ è½½Chronosæ¨¡å‹...")
                self.chronos_pipeline = ChronosPipeline.from_pretrained(
                    "amazon/chronos-t5-small",
                    device_map=self.device,
                    torch_dtype=torch.bfloat16,
                )
                print("  âœ… Chronosæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"  âŒ Chronosæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.chronos_pipeline = None
        
        # åˆå§‹åŒ–MomentFM (å¦‚æœå¯ç”¨)
        if MOMENT_AVAILABLE:
            try:
                print("  åŠ è½½MomentFMæ¨¡å‹...")
                self.moment_pipeline = MOMENTPipeline.from_pretrained(
                    "AutonLab/MOMENT-1-large", 
                    model_kwargs={'task_name': 'embedding'}
                )
                print("  âœ… MomentFMæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"  âŒ MomentFMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.moment_pipeline = None
    
    def load_motor_data(self) -> Tuple[List[np.ndarray], List[str]]:
        """åŠ è½½ç”µæœºæ•°æ®"""
        print("ğŸ“‚ åŠ è½½ç”µæœºæ•°æ®...")
        
        # ä»å¢å¼ºæ•°æ®åŠ è½½å™¨åŠ è½½æ•°æ®
        import sys
        sys.path.append(str(Path(__file__).parent.parent / 'data_processing'))
        from enhanced_data_loader import EnhancedMotorDataLoader
        
        config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
        loader = EnhancedMotorDataLoader(str(config_path))
        dataset, _ = loader.load_comprehensive_dataset(enable_augmentation=False)
        
        # æ”¶é›†æ‰€æœ‰ä¿¡å·
        all_signals = []
        all_labels = []
        
        for state in ['normal', 'spark', 'vibrate']:
            signals = dataset['single_sensor'][state][:100]  # æ¯ç±»å–100ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
            all_signals.extend(signals)
            all_labels.extend([state] * len(signals))
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(all_signals)} ä¸ªä¿¡å·")
        return all_signals, all_labels
    
    def prepare_time_series_data(self, signals: List[np.ndarray], 
                               context_length: int = 512, 
                               prediction_length: int = 64) -> List[Dict]:
        """å‡†å¤‡æ—¶åºæ•°æ®"""
        print(f"ğŸ”§ å‡†å¤‡æ—¶åºæ•°æ® (ä¸Šä¸‹æ–‡é•¿åº¦: {context_length}, é¢„æµ‹é•¿åº¦: {prediction_length})...")
        
        prepared_data = []
        
        for i, signal in enumerate(signals):
            # ç¡®ä¿ä¿¡å·è¶³å¤Ÿé•¿
            min_length = context_length + prediction_length
            if len(signal) < min_length:
                # å¡«å……ä¿¡å·
                signal = np.pad(signal, (0, min_length - len(signal)), 'reflect')
            
            # æ ‡å‡†åŒ–
            signal_mean = np.mean(signal)
            signal_std = np.std(signal) + 1e-8
            signal_normalized = (signal - signal_mean) / signal_std
            
            # åˆ›å»ºæ»‘åŠ¨çª—å£
            for start_idx in range(0, len(signal_normalized) - min_length + 1, prediction_length):
                end_context = start_idx + context_length
                end_prediction = end_context + prediction_length
                
                if end_prediction <= len(signal_normalized):
                    context = signal_normalized[start_idx:end_context]
                    target = signal_normalized[end_context:end_prediction]
                    
                    prepared_data.append({
                        'signal_id': i,
                        'context': context,
                        'target': target,
                        'mean': signal_mean,
                        'std': signal_std
                    })
        
        print(f"âœ… å‡†å¤‡å®Œæˆ: {len(prepared_data)} ä¸ªæ—¶åºç‰‡æ®µ")
        return prepared_data
    
    def extract_moment_embeddings(self, contexts: List[np.ndarray]) -> Optional[np.ndarray]:
        """ä½¿ç”¨MomentFMæå–åµŒå…¥ç‰¹å¾"""
        if not self.moment_pipeline:
            print("âš ï¸ MomentFMä¸å¯ç”¨ï¼Œè·³è¿‡åµŒå…¥æå–")
            return None
        
        print("ğŸ”§ æå–MomentFMåµŒå…¥...")
        
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = []
            for context in contexts:
                # MomentFMæœŸæœ›çš„è¾“å…¥æ ¼å¼
                input_data.append(torch.tensor(context, dtype=torch.float32).unsqueeze(0))
            
            # æ‰¹é‡å¤„ç†
            embeddings = []
            batch_size = 32
            
            for i in range(0, len(input_data), batch_size):
                batch = input_data[i:i+batch_size]
                batch_tensor = torch.stack(batch).squeeze(1)  # [batch_size, seq_len]
                
                with torch.no_grad():
                    batch_embeddings = self.moment_pipeline(batch_tensor)
                    embeddings.append(batch_embeddings.cpu().numpy())
            
            embeddings = np.concatenate(embeddings, axis=0)
            print(f"âœ… åµŒå…¥æå–å®Œæˆ: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            print(f"âŒ MomentFMåµŒå…¥æå–å¤±è´¥: {e}")
            return None
    
    def chronos_predict(self, contexts: List[np.ndarray], 
                       prediction_length: int = 64) -> Optional[np.ndarray]:
        """ä½¿ç”¨Chronosè¿›è¡Œé¢„æµ‹"""
        if not self.chronos_pipeline:
            print("âš ï¸ Chronosä¸å¯ç”¨ï¼Œè·³è¿‡é¢„æµ‹")
            return None
        
        print("ğŸ”® Chronosé¢„æµ‹...")
        
        try:
            predictions = []
            batch_size = 16
            
            for i in range(0, len(contexts), batch_size):
                batch_contexts = contexts[i:i+batch_size]
                
                # è½¬æ¢ä¸ºtensor
                batch_tensor = torch.tensor(np.array(batch_contexts), dtype=torch.float32)
                
                # Chronosé¢„æµ‹
                with torch.no_grad():
                    forecast = self.chronos_pipeline.predict(
                        context=batch_tensor,
                        prediction_length=prediction_length,
                        num_samples=20  # ç”Ÿæˆå¤šä¸ªæ ·æœ¬ä»¥è·å¾—ä¸ç¡®å®šæ€§
                    )
                
                predictions.append(forecast.cpu().numpy())
            
            predictions = np.concatenate(predictions, axis=0)
            print(f"âœ… é¢„æµ‹å®Œæˆ: {predictions.shape}")
            return predictions
            
        except Exception as e:
            print(f"âŒ Chronosé¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def detect_anomalies(self, targets: List[np.ndarray], 
                        predictions: np.ndarray, 
                        confidence_level: float = 0.95) -> np.ndarray:
        """æ£€æµ‹å¼‚å¸¸"""
        print("ğŸ” æ£€æµ‹å¼‚å¸¸...")
        
        anomaly_scores = []
        
        for i, (target, pred_samples) in enumerate(zip(targets, predictions)):
            # è®¡ç®—é¢„æµ‹åˆ†å¸ƒçš„ç»Ÿè®¡é‡
            pred_mean = np.mean(pred_samples, axis=0)
            pred_std = np.std(pred_samples, axis=0)
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            z_score = 1.96 if confidence_level == 0.95 else 2.58  # 95% or 99%
            lower_bound = pred_mean - z_score * pred_std
            upper_bound = pred_mean + z_score * pred_std
            
            # æ£€æŸ¥ç›®æ ‡å€¼æ˜¯å¦åœ¨ç½®ä¿¡åŒºé—´å†…
            in_bounds = (target >= lower_bound) & (target <= upper_bound)
            anomaly_ratio = 1 - np.mean(in_bounds)
            
            # è®¡ç®—æ®‹å·®
            residual = np.mean(np.abs(target - pred_mean))
            
            # ç»¼åˆå¼‚å¸¸åˆ†æ•°
            anomaly_score = anomaly_ratio * 0.7 + min(residual, 1.0) * 0.3
            anomaly_scores.append(anomaly_score)
        
        anomaly_scores = np.array(anomaly_scores)
        
        # ç¡®å®šå¼‚å¸¸é˜ˆå€¼
        threshold = np.percentile(anomaly_scores, 90)  # å‰10%ä¸ºå¼‚å¸¸
        anomalies = anomaly_scores > threshold
        
        print(f"âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆ: å‘ç° {np.sum(anomalies)} ä¸ªå¼‚å¸¸ (æ€»å…± {len(anomalies)} ä¸ªæ ·æœ¬)")
        
        return anomalies, anomaly_scores
    
    def run_anomaly_detection_experiment(self) -> Dict:
        """è¿è¡Œå¼‚å¸¸æ£€æµ‹å®éªŒ"""
        print("ğŸš€ å¼€å§‹MomentFM + Chronoså¼‚å¸¸æ£€æµ‹å®éªŒ")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        signals, labels = self.load_motor_data()
        
        # å‡†å¤‡æ—¶åºæ•°æ®
        prepared_data = self.prepare_time_series_data(signals)
        
        if len(prepared_data) == 0:
            print("âŒ æ²¡æœ‰å‡†å¤‡å¥½çš„æ•°æ®")
            return {}
        
        # æå–ä¸Šä¸‹æ–‡å’Œç›®æ ‡
        contexts = [item['context'] for item in prepared_data]
        targets = [item['target'] for item in prepared_data]
        signal_ids = [item['signal_id'] for item in prepared_data]
        
        # æå–MomentFMåµŒå…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        embeddings = self.extract_moment_embeddings(contexts)
        
        # Chronosé¢„æµ‹
        predictions = self.chronos_predict(contexts)
        
        if predictions is None:
            print("âŒ é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åŸºçº¿æ–¹æ³•")
            return self.baseline_anomaly_detection(signals, labels)
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies, anomaly_scores = self.detect_anomalies(targets, predictions)
        
        # è¯„ä¼°ç»“æœ
        results = self._evaluate_anomaly_detection(signal_ids, anomalies, anomaly_scores, labels)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, prepared_data, anomalies, anomaly_scores)
        
        return results

    def _evaluate_anomaly_detection(self, signal_ids: List[int], anomalies: np.ndarray,
                                   anomaly_scores: np.ndarray, labels: List[str]) -> Dict:
        """è¯„ä¼°å¼‚å¸¸æ£€æµ‹ç»“æœ"""
        print("ğŸ“Š è¯„ä¼°å¼‚å¸¸æ£€æµ‹ç»“æœ...")

        # å°†ç‰‡æ®µçº§åˆ«çš„ç»“æœèšåˆåˆ°ä¿¡å·çº§åˆ«
        signal_anomaly_scores = {}
        signal_labels = {}

        for i, (signal_id, anomaly, score) in enumerate(zip(signal_ids, anomalies, anomaly_scores)):
            if signal_id not in signal_anomaly_scores:
                signal_anomaly_scores[signal_id] = []
                signal_labels[signal_id] = labels[signal_id]

            signal_anomaly_scores[signal_id].append(score)

        # è®¡ç®—æ¯ä¸ªä¿¡å·çš„å¹³å‡å¼‚å¸¸åˆ†æ•°
        signal_final_scores = {}
        signal_final_predictions = {}

        for signal_id, scores in signal_anomaly_scores.items():
            avg_score = np.mean(scores)
            signal_final_scores[signal_id] = avg_score
            # å¦‚æœå¹³å‡åˆ†æ•°è¶…è¿‡é˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºæ˜¯å¼‚å¸¸
            signal_final_predictions[signal_id] = avg_score > np.percentile(list(signal_anomaly_scores.values()), 70)

        # è½¬æ¢ä¸ºè¯„ä¼°æ ¼å¼
        true_labels = []
        pred_labels = []

        for signal_id in sorted(signal_final_scores.keys()):
            true_label = 0 if signal_labels[signal_id] == 'normal' else 1
            pred_label = 1 if signal_final_predictions[signal_id] else 0

            true_labels.append(true_label)
            pred_labels.append(pred_label)

        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

        accuracy = accuracy_score(true_labels, pred_labels)
        f1 = f1_score(true_labels, pred_labels, average='weighted')
        precision = precision_score(true_labels, pred_labels, average='weighted')
        recall = recall_score(true_labels, pred_labels, average='weighted')

        results = {
            'method': 'MomentFM_Chronos',
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall,
            'anomaly_count': sum(pred_labels),
            'total_samples': len(pred_labels),
            'signal_scores': signal_final_scores,
            'signal_predictions': signal_final_predictions,
            'signal_labels': signal_labels
        }

        print(f"âœ… è¯„ä¼°å®Œæˆ:")
        print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"   F1åˆ†æ•°: {f1:.4f}")
        print(f"   ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"   å¬å›ç‡: {recall:.4f}")
        print(f"   å¼‚å¸¸æ•°é‡: {sum(pred_labels)}/{len(pred_labels)}")

        return results

    def _save_results(self, results: Dict, prepared_data: List[Dict],
                     anomalies: np.ndarray, anomaly_scores: np.ndarray):
        """ä¿å­˜ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")

        # ä¿å­˜ä¸»è¦ç»“æœ
        results_df = pd.DataFrame([results])
        results_path = self.output_path / 'moment_chronos_results.csv'
        results_df.to_csv(results_path, index=False)

        # ä¿å­˜è¯¦ç»†çš„å¼‚å¸¸åˆ†æ•°
        detailed_results = []
        for i, (data, anomaly, score) in enumerate(zip(prepared_data, anomalies, anomaly_scores)):
            detailed_results.append({
                'segment_id': i,
                'signal_id': data['signal_id'],
                'anomaly_score': score,
                'is_anomaly': anomaly,
                'context_mean': np.mean(data['context']),
                'context_std': np.std(data['context']),
                'target_mean': np.mean(data['target']),
                'target_std': np.std(data['target'])
            })

        detailed_df = pd.DataFrame(detailed_results)
        detailed_path = self.output_path / 'moment_chronos_detailed_results.csv'
        detailed_df.to_csv(detailed_path, index=False)

        # ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
        json_results = {
            'timestamp': datetime.now().isoformat(),
            'summary': results,
            'model_info': {
                'chronos_available': CHRONOS_AVAILABLE,
                'moment_available': MOMENT_AVAILABLE,
                'device': str(self.device)
            }
        }

        json_path = self.output_path / 'moment_chronos_experiment.json'
        with open(json_path, 'w') as f:
            json.dump(json_results, f, indent=2, default=str)

        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜:")
        print(f"   ä¸»è¦ç»“æœ: {results_path}")
        print(f"   è¯¦ç»†ç»“æœ: {detailed_path}")
        print(f"   å®Œæ•´å®éªŒ: {json_path}")

def run_moment_chronos_experiment():
    """è¿è¡ŒMomentFM + Chronoså®éªŒ"""
    print("ğŸš€ å¯åŠ¨MomentFM + Chronoså¼‚å¸¸æ£€æµ‹å®éªŒ")

    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"

    # åˆ›å»ºæ£€æµ‹å™¨
    detector = MomentChronosAnomalyDetector(str(config_path))

    # è¿è¡Œå®éªŒ
    results = detector.run_anomaly_detection_experiment()

    return results

if __name__ == "__main__":
    # è¿è¡Œå®éªŒ
    results = run_moment_chronos_experiment()

    if results:
        print("\nğŸ‰ MomentFM + Chronoså®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   æ–¹æ³•: {results['method']}")
        print(f"   å‡†ç¡®ç‡: {results['accuracy']:.4f}")
        print(f"   F1åˆ†æ•°: {results['f1']:.4f}")
    else:
        print("âŒ å®éªŒå¤±è´¥")
    
    def baseline_anomaly_detection(self, signals: List[np.ndarray], labels: List[str]) -> Dict:
        """åŸºçº¿å¼‚å¸¸æ£€æµ‹æ–¹æ³•"""
        print("ğŸ”§ ä½¿ç”¨åŸºçº¿å¼‚å¸¸æ£€æµ‹æ–¹æ³•...")
        
        from sklearn.ensemble import IsolationForest
        from sklearn.preprocessing import StandardScaler
        from sklearn.metrics import accuracy_score, f1_score
        
        # æå–ç®€å•ç»Ÿè®¡ç‰¹å¾
        features = []
        for signal in signals:
            feat = [
                np.mean(signal), np.std(signal), np.var(signal),
                np.min(signal), np.max(signal), np.median(signal),
                np.percentile(signal, 25), np.percentile(signal, 75)
            ]
            features.append(feat)
        
        features = np.array(features)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # å­¤ç«‹æ£®æ—
        iso_forest = IsolationForest(contamination=0.2, random_state=42)
        anomaly_pred = iso_forest.fit_predict(features_scaled)
        
        # è½¬æ¢æ ‡ç­¾
        true_labels = [0 if label == 'normal' else 1 for label in labels]
        pred_labels = [1 if pred == -1 else 0 for pred in anomaly_pred]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, pred_labels)
        f1 = f1_score(true_labels, pred_labels, average='weighted')
        
        results = {
            'method': 'Baseline_IsolationForest',
            'accuracy': accuracy,
            'f1': f1,
            'anomaly_count': sum(pred_labels),
            'total_samples': len(pred_labels)
        }
        
        print(f"âœ… åŸºçº¿æ–¹æ³•å®Œæˆ: å‡†ç¡®ç‡ {accuracy:.4f}, F1 {f1:.4f}")
        
        return results
