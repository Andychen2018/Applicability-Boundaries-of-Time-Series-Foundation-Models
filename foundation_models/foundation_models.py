#!/usr/bin/env python3
"""
æ—¶åºåŸºç¡€æ¨¡å‹å®éªŒ
åŒ…å«é¢„è®­ç»ƒæ¨¡å‹å’Œè¿ç§»å­¦ä¹ æ–¹æ³•
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from pathlib import Path
import yaml
import json
import gc
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class TimeSeriesFoundationDataset(Dataset):
    """æ—¶åºåŸºç¡€æ¨¡å‹æ•°æ®é›†"""
    
    def __init__(self, signals: List[np.ndarray], labels: np.ndarray, 
                 max_length: int = 1024, normalize: bool = True):
        self.labels = labels
        self.max_length = max_length
        
        # é¢„å¤„ç†ä¿¡å·
        self.processed_signals = []
        for signal in signals:
            # é‡é‡‡æ ·åˆ°å›ºå®šé•¿åº¦
            if len(signal) > max_length:
                # ç­‰é—´éš”é‡‡æ ·
                indices = np.linspace(0, len(signal)-1, max_length, dtype=int)
                signal = signal[indices]
            else:
                # å¡«å……
                signal = np.pad(signal, (0, max_length - len(signal)), 'constant')
            
            # æ ‡å‡†åŒ–
            if normalize:
                signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)
            
            self.processed_signals.append(signal)
        
        print(f"ğŸ“Š åŸºç¡€æ¨¡å‹æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(self.processed_signals)} ä¸ªä¿¡å·ï¼Œé•¿åº¦ {max_length}")
    
    def __len__(self):
        return len(self.processed_signals)
    
    def __getitem__(self, idx):
        signal = self.processed_signals[idx]
        label = self.labels[idx]
        return torch.FloatTensor(signal), torch.LongTensor([label])

class PretrainedTimeSeriesEncoder(nn.Module):
    """é¢„è®­ç»ƒæ—¶åºç¼–ç å™¨"""
    
    def __init__(self, input_length: int = 1024, d_model: int = 256, 
                 nhead: int = 8, num_layers: int = 6):
        super(PretrainedTimeSeriesEncoder, self).__init__()
        
        self.d_model = d_model
        self.input_length = input_length
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(1, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = self._create_positional_encoding(input_length, d_model)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,
            dropout=0.1, batch_first=True, activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(d_model)
        
    def _create_positional_encoding(self, max_len: int, d_model: int):
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return nn.Parameter(pe.unsqueeze(0), requires_grad=False)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len)
        batch_size, seq_len = x.shape
        
        # æ·»åŠ ç‰¹å¾ç»´åº¦
        x = x.unsqueeze(-1)  # (batch_size, seq_len, 1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:, :seq_len, :]
        
        # Transformerç¼–ç 
        x = self.transformer(x)
        
        # å±‚å½’ä¸€åŒ–
        x = self.layer_norm(x)
        
        return x

class FoundationModelClassifier(nn.Module):
    """åŸºäºåŸºç¡€æ¨¡å‹çš„åˆ†ç±»å™¨"""
    
    def __init__(self, input_length: int = 1024, d_model: int = 256, 
                 num_classes: int = 3, freeze_encoder: bool = False):
        super(FoundationModelClassifier, self).__init__()
        
        # é¢„è®­ç»ƒç¼–ç å™¨
        self.encoder = PretrainedTimeSeriesEncoder(input_length, d_model)
        
        # æ˜¯å¦å†»ç»“ç¼–ç å™¨
        if freeze_encoder:
            for param in self.encoder.parameters():
                param.requires_grad = False
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(d_model, 128),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        # ç¼–ç 
        encoded = self.encoder(x)  # (batch_size, seq_len, d_model)
        
        # è½¬ç½®ç”¨äºæ± åŒ–
        encoded = encoded.transpose(1, 2)  # (batch_size, d_model, seq_len)
        
        # åˆ†ç±»
        output = self.classifier(encoded)
        
        return output

class ContrastiveLearningModel(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, input_length: int = 1024, d_model: int = 128):
        super(ContrastiveLearningModel, self).__init__()
        
        # ç¼–ç å™¨
        self.encoder = PretrainedTimeSeriesEncoder(input_length, d_model, nhead=4, num_layers=3)
        
        # æŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(d_model, 64),
            nn.GELU(),
            nn.Linear(64, 32)
        )
    
    def forward(self, x):
        # ç¼–ç 
        encoded = self.encoder(x)
        
        # è½¬ç½®ç”¨äºæ± åŒ–
        encoded = encoded.transpose(1, 2)
        
        # æŠ•å½±
        projected = self.projection_head(encoded)
        
        # L2å½’ä¸€åŒ–
        projected = nn.functional.normalize(projected, dim=1)
        
        return projected

class FoundationModelPipeline:
    """åŸºç¡€æ¨¡å‹æµæ°´çº¿"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.output_path = Path(self.config['output']['tables'])
        self.models_path = self.output_path.parent / 'models'
        self.models_path.mkdir(exist_ok=True)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.6)  # ä½¿ç”¨60%çš„GPUå†…å­˜
        
        self.results = {}
    
    def clear_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def load_and_prepare_data(self) -> Tuple:
        """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
        print("ğŸ“‚ åŠ è½½åŸºç¡€æ¨¡å‹æ•°æ®...")
        
        # ä»data_processingæ¨¡å—åŠ è½½æ•°æ®
        import sys
        sys.path.append(str(Path(__file__).parent.parent / 'data_processing'))
        from data_loader import MotorDataLoader
        
        config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"
        loader = MotorDataLoader(str(config_path))
        data, _ = loader.load_all_data(max_files_per_state=25)  # å‡å°‘æ•°æ®é‡
        
        # æ•´ç†æ•°æ®
        all_signals = []
        all_labels = []
        
        label_map = {'normal': 0, 'spark': 1, 'vibrate': 2}
        
        for sensor in data.keys():
            for state in data[sensor].keys():
                signals = data[sensor][state]
                for signal in signals:
                    all_signals.append(signal)
                    all_labels.append(label_map[state])
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(all_signals)} ä¸ªä¿¡å·")
        
        # æ•°æ®åˆ’åˆ†
        X_temp, X_test, y_temp, y_test = train_test_split(
            all_signals, all_labels, test_size=0.2, random_state=42, stratify=all_labels
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
        )
        
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ†: è®­ç»ƒ{len(X_train)}, éªŒè¯{len(X_val)}, æµ‹è¯•{len(X_test)}")
        
        return X_train, X_val, X_test, np.array(y_train), np.array(y_val), np.array(y_test)
    
    def create_data_loaders(self, X_train, X_val, X_test, y_train, y_val, y_test, 
                           batch_size: int = 4) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TimeSeriesFoundationDataset(X_train, y_train, max_length=1024)
        val_dataset = TimeSeriesFoundationDataset(X_val, y_val, max_length=1024)
        test_dataset = TimeSeriesFoundationDataset(X_test, y_test, max_length=1024)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        
        return train_loader, val_loader, test_loader

    def train_foundation_model(self, train_loader: DataLoader, val_loader: DataLoader,
                              model_name: str = "FoundationModel") -> Dict:
        """è®­ç»ƒåŸºç¡€æ¨¡å‹"""
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ {model_name}")
        print("-" * 40)

        # æ¸…ç†å†…å­˜
        self.clear_memory()

        # åˆ›å»ºæ¨¡å‹
        if model_name == "FoundationModel":
            model = FoundationModelClassifier(input_length=1024, d_model=128, freeze_encoder=False)
        elif model_name == "FineTunedModel":
            model = FoundationModelClassifier(input_length=1024, d_model=128, freeze_encoder=True)
        else:
            model = FoundationModelClassifier(input_length=1024, d_model=64, freeze_encoder=False)

        model = model.to(self.device)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)

        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        val_accuracies = []

        best_val_acc = 0.0
        best_model_state = None
        patience_counter = 0

        epochs = 25

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0

            for batch_idx, (batch_signals, batch_labels) in enumerate(train_loader):
                batch_signals = batch_signals.to(self.device)
                batch_labels = batch_labels.squeeze().to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_signals)
                loss = criterion(outputs, batch_labels)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()
                train_loss += loss.item()

                # æ˜¾ç¤ºè¿›åº¦
                if batch_idx % 5 == 0:
                    print(f"  Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")

            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            correct = 0
            total = 0

            with torch.no_grad():
                for batch_signals, batch_labels in val_loader:
                    batch_signals = batch_signals.to(self.device)
                    batch_labels = batch_labels.squeeze().to(self.device)

                    outputs = model(batch_signals)
                    loss = criterion(outputs, batch_labels)
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs.data, 1)
                    total += batch_labels.size(0)
                    correct += (predicted == batch_labels).sum().item()

            # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            val_accuracy = correct / total

            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_accuracy)

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()

            # æ—©åœæ£€æŸ¥
            if val_accuracy > best_val_acc:
                best_val_acc = val_accuracy
                best_model_state = model.state_dict().copy()
                patience_counter = 0
            else:
                patience_counter += 1

            print(f"  Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, "
                  f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

            # æ—©åœ
            if patience_counter >= 5:
                print(f"  æ—©åœäº epoch {epoch}")
                break

            # æ¸…ç†å†…å­˜
            if epoch % 3 == 0:
                self.clear_memory()

        # æ¢å¤æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)

        # ä¿å­˜æ¨¡å‹
        model_path = self.models_path / f'{model_name.lower()}_foundation.pth'
        torch.save(model.state_dict(), model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")

        training_history = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies,
            'best_val_acc': best_val_acc,
            'model': model
        }

        print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")

        return training_history

    def evaluate_model(self, model: nn.Module, test_loader: DataLoader) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        model.eval()
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch_signals, batch_labels in test_loader:
                batch_signals = batch_signals.to(self.device)
                batch_labels = batch_labels.squeeze().to(self.device)

                outputs = model(batch_signals)
                _, predicted = torch.max(outputs, 1)

                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(batch_labels.cpu().numpy())

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')

        return {
            'accuracy': accuracy,
            'f1': f1,
            'predictions': all_predictions,
            'labels': all_labels
        }

    def run_foundation_experiments(self) -> Dict:
        """è¿è¡ŒåŸºç¡€æ¨¡å‹å®éªŒ"""
        print("ğŸš€ å¼€å§‹åŸºç¡€æ¨¡å‹å®éªŒ")
        print("=" * 50)

        # åŠ è½½æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test = self.load_and_prepare_data()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = self.create_data_loaders(
            X_train, X_val, X_test, y_train, y_val, y_test, batch_size=4
        )

        # å®éªŒé…ç½®
        experiments = [
            "FoundationModel",
            "CompactFoundation"
        ]

        all_results = {}

        for model_name in experiments:
            try:
                print(f"\n{'='*20} {model_name} {'='*20}")

                # è®­ç»ƒæ¨¡å‹
                training_history = self.train_foundation_model(train_loader, val_loader, model_name)

                # æµ‹è¯•é›†è¯„ä¼°
                test_metrics = self.evaluate_model(training_history['model'], test_loader)

                all_results[model_name] = {
                    'training_history': training_history,
                    'test_metrics': test_metrics
                }

                print(f"ğŸ‰ {model_name} - æµ‹è¯•å‡†ç¡®ç‡: {test_metrics['accuracy']:.4f}, F1: {test_metrics['f1']:.4f}")

                # æ¸…ç†å†…å­˜
                del training_history['model']
                self.clear_memory()

            except Exception as e:
                print(f"âŒ {model_name} å®éªŒå¤±è´¥: {e}")
                self.clear_memory()
                continue

        self.results = all_results
        return all_results

    def save_results(self) -> pd.DataFrame:
        """ä¿å­˜ç»“æœ"""
        print("\nğŸ’¾ ä¿å­˜åŸºç¡€æ¨¡å‹ç»“æœ...")

        # æ•´ç†ç»“æœæ•°æ®
        results_data = []

        for model_name, result in self.results.items():
            test_metrics = result['test_metrics']

            result_row = {
                'model': model_name,
                'test_accuracy': test_metrics['accuracy'],
                'test_f1': test_metrics['f1'],
                'best_val_acc': result['training_history']['best_val_acc']
            }
            results_data.append(result_row)

        # ä¿å­˜ä¸ºCSV
        results_df = pd.DataFrame(results_data)
        results_path = self.output_path / 'foundation_models_results.csv'
        results_df.to_csv(results_path, index=False)

        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜: {results_path}")

        return results_df

if __name__ == "__main__":
    # æµ‹è¯•åŸºç¡€æ¨¡å‹æµæ°´çº¿
    config_path = Path(__file__).parent.parent.parent / "experiments/configs/config.yaml"

    # åˆ›å»ºæµæ°´çº¿
    pipeline = FoundationModelPipeline(str(config_path))

    # è¿è¡Œå®éªŒ
    results = pipeline.run_foundation_experiments()

    # ä¿å­˜ç»“æœ
    results_df = pipeline.save_results()

    print("\nğŸ‰ åŸºç¡€æ¨¡å‹å®éªŒå®Œæˆï¼")
    if len(results_df) > 0:
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½æ’åº (æŒ‰æµ‹è¯•F1åˆ†æ•°):")
        print(results_df.sort_values('test_f1', ascending=False)[['model', 'test_accuracy', 'test_f1']].to_string(index=False))
